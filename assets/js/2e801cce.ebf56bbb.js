"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[116],{6029:n=>{n.exports=JSON.parse('{"blogPosts":[{"id":"littlebird","metadata":{"permalink":"/littlebird","source":"@site/i18n/en/docusaurus-plugin-content-blog/2023-04-18-littlebird/index.mdx","title":"LittleBird: Efficient Faster & Longer Transformer for Question Answering","description":"Describing the How to implement the LittleBird sparse attention transformer for question answering","date":"2023-04-18T00:00:00.000Z","formattedDate":"April 18, 2023","tags":[{"label":"nlp","permalink":"/tags/nlp"},{"label":"koreannlp","permalink":"/tags/koreannlp"},{"label":"deeplearning","permalink":"/tags/deeplearning"},{"label":"sparse attention","permalink":"/tags/sparse-attention"},{"label":"seq2seq","permalink":"/tags/seq-2-seq"},{"label":"transformer variant","permalink":"/tags/transformer-variant"},{"label":"paper implementation","permalink":"/tags/paper-implementation"}],"readingTime":15.765,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"littlebird","title":"LittleBird: Efficient Faster & Longer Transformer for Question Answering","tags":["nlp","koreannlp","deeplearning","sparse attention","seq2seq","transformer variant","paper implementation"],"description":"Describing the How to implement the LittleBird sparse attention transformer for question answering","keywords":["nlp","transformers","koreannlp","korquad","deeplearning","sparse attention","littlebird"]}},"content":"import Figure from \\"../../../../src/components/figure\\";\\nimport full_sparse_attn_img from \\"../../../../static/img/2023-04-18-littlebird/full_sparse_attn.png\\";\\nimport block_tokens_ex_img from \\"../../../../static/img/2023-04-18-littlebird/blocked_tokens_example.png\\";\\nimport first_two_rows_img from \\"../../../../static/img/2023-04-18-littlebird/first_two_rows.png\\";\\nimport middle_rows_img from \\"../../../../static/img/2023-04-18-littlebird/middle_rows.png\\";\\nimport last_row_img from \\"../../../../static/img/2023-04-18-littlebird/last_row.png\\";\\nimport sliding_window_broken_down from \\"../../../../static/img/2023-04-18-littlebird/sliding_window_broken_down.png\\";\\n\\nIn this post I will try to explain the overall structure as well as how I went about implementing the model described in the LittleBird<sub>[1]</sub> paper.\\n\\n![little bird layer](../../../../static/img/2023-04-18-littlebird/littlebird_layer.png)\\n\\n## Intro\\n\\nLittleBird is a sparse attention model proposed by Kakao Enterprise Corp. that improves on BigBird by reducing the memory footprint and improving the speed while maintaining accuracy. The model is a combination of BigBird\'s sliding window attention and LUNA pack and unpack attention with a custom bi-directional positional representation method based on ALiBi. As of 2022.03.08 the model sits at first place on the KorQuad 2.0 test set.\\n\\nLet\'s begin by taking a look at the various formulas shown in the paper to get an idea of how one may go about implemnting the model.\\n\\n:::note\\nIn this post I focus almost exclusively on my implementation of the LittleBird model. For those curious about the training methods used the by authors, or the more theoretical aspects of the model, I recommend reading the paper.\\n:::\\n\\n## Formulas\\n\\n### LittleBird Layer\\n\\n$$\\n\\\\begin{aligned}\\nC_p = Attn(P,X) \\\\\\\\\\nP^{\\\\prime} = LayerNorm(C_p + P) \\\\\\\\\\nC_x = USWAttn(X, C_p) \\\\\\\\\\nA = LayerNorm(C_x + X) \\\\\\\\\\nX^{\\\\prime} = LayerNorm(FFN(A) + A)\\n\\\\end{aligned}\\n$$\\n\\nwhere $X \\\\in \\\\R^{l \\\\text{ x } d}$ is the input sequence, and $l$ and $d$\ub294 are the sequence length and embedding dimension respectively. $P \\\\in \\\\R^{s \\\\text{ x } d}$ is Pack Attention\'s projection matrix, and $s$ is the length of the packed sequence.\\n\\n### Attention\\n\\n$$\\nAttn(X,C) = \\\\sigma(\\\\frac{Q(X)K(C)^T}{\\\\sqrt{d}})V(C)\\n$$\\n\\n$$\\nUSWAttn(X, C_p) = \\\\\\\\\\n\\\\begin{aligned}\\n\\\\sigma(\\\\frac{Q(X)[K(C_P);K(X)]^T}{\\\\sqrt{d}} - [D_p;D]^T) \\\\\\\\ \\\\cdot [V(C_p);V(X)]\\n\\\\end{aligned}\\n$$\\n\\nwhere $[A;B]$ is the row-wise concatenation of $A$ and $B$, and $USWAttn$ refers to Unpack & Sliding Window Attention.\\n\\n### BiALiBi\\n\\n$$\\nD_p = (\\n    \\\\frac{\\\\beta + \\\\gamma}{2}b)J_{s,l}\\n$$\\n\\n$$\\nD_{i,j} = \\\\begin{cases}\\n    0, &\\\\text{for } i=j\\\\\\\\\\n    \\\\alpha, &\\\\text{for } i=0 &\\\\text{or } j=0\\\\\\\\\\n    \\\\beta(i-j), &\\\\text{for } i>j\\\\\\\\\\n    \\\\gamma(j-i), &\\\\text{for } i<j\\n\\\\end{cases}\\n$$\\n\\nwhere $D_p \\\\in \\\\R^{s \\\\text{ x } l}$ is the distance function for representing positional data for Pack Attention, and $J_{s,l}$ is all-ones matrix with size $s$ x $l$.\\n$D_{i,j}$is the distance function, named BiALiBi by the authors, used to represent the positional information for LittleBird\'s Sliding Window Attention. $\\\\alpha$, $\\\\beta$, $\\\\gamma$ are all trainable parameters.\\n\\n## Hyperparmaters\\n\\nIt\'s a good idea to know what the hyperparameters used in the model are, so I listed them in the table below. These will also be referenced in the various code samples below.\\n\\n| variable            | dtype | default value | description                                                |\\n| :------------------ | :---- | :-----------: | :--------------------------------------------------------- |\\n| seq_len             | int   |     None      | length of the input sequence                               |\\n| pack_len            | int   |     None      | length of the project matrix                               |\\n| d_model             | int   |      512      | embedding size                                             |\\n| d_ff                | int   |     2048      | FeedForward layer size                                     |\\n| num_attention_heads | int   |       8       | -                                                          |\\n| num_heads           | int   |       8       | same as num_attention_heads                                |\\n| dropout_p           | float |      0.1      | -                                                          |\\n| block_size          | int   |      64       | block size used for calculating USWAttention               |\\n| window_size         | int   |       3       | window size used when calculating Sliding Window Attention |\\n\\n## LittleBirdLayer\\n\\nI would like to focus on building the LittleBirdModel using a top-down approach starting with the LittleBirdLayer. As shown in the formula above, the layer is\\ncomposed of three LayerNorms, a FeedForward layer, a MultiHeadAttention layer, and the USWAttn layer. This is pretty easy to implement and we can do so as shown below.\\n\\n### init\\n\\n```python\\nclass LittleBirdLayer(nn.Module):\\n    def __init__(...) -> None:\\n        ...\\n\\n        self.pack_attn = PackAttention(d_model, num_attention_heads)\\n        self.unpack_sliding_attn = UnpackSlidingWindowAttention(\\n            seq_len, pack_len, d_model, num_attention_heads, block_size\\n        )\\n        self.feed_forward = PositionwiseFeedForwardNetwork(d_model, d_ff, dropout_p)\\n\\n        self.pack_attn_layer_norm = nn.LayerNorm(d_model)\\n        self.unpack_sliding_attn_layer_norm = nn.LayerNorm(d_model)\\n        self.ffn_layer_norm = nn.LayerNorm(d_model)\\n```\\n\\n### Forward\\n\\nSince we have conveniently abstracted the work to their respective layers, we can implement the forward pass almost exactly as it is written in the formula above.\\n\\nOur model will have multiple layers, so we need to return both $P^{\\\\prime}$ and $X^{\\\\prime}$ as we will pass them directly into the next layer as $P$ and $X$.\\n\\n```python\\n    def forward(\\n        self,\\n        P: torch.Tensor,\\n        X: torch.Tensor,\\n        attention_mask: torch.Tensor\\n    ):\\n        Cp = self.pack_attn(P, X, attention_mask)\\n        P0 = self.pack_attn_layer_norm(Cp + P)\\n\\n        Cx = self.unpack_sliding_attn(X, Cp, attention_mask)\\n        A = self.unpack_sliding_attn_layer_norm(Cx + X)\\n\\n        X0 = self.ffn_layer_norm(self.feed_forward(A) + A)\\n\\n        return P0, X0\\n```\\n\\n## Feed Forward\\n\\nWith transformers, it\'s common to add non-linearity to the model using a FeedForward layer. The usage of this particular class is shown in the code sample directly above.\\n\\nHere we just implement the layer as described in the original transformer paper.\\n\\n```python\\nclass PositionwiseFeedForwardNetwork(nn.Module):\\n    def __init__(...) -> None:\\n        super(PositionwiseFeedForwardNetwork, self).__init__()\\n        self.feed_forward = nn.Sequential(\\n            nn.Linear(d_model, d_ff),\\n            nn.Dropout(dropout_p),\\n            nn.ReLU(),\\n            nn.Linear(d_ff, d_model),\\n            nn.Dropout(dropout_p),\\n        )\\n\\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\\n        return self.feed_forward(inputs)\\n```\\n\\n## PackAttention\\n\\nThe pack part of Pack and Unpack Attention $C_p$ is the same as multihead attention from the original transformer model. We can just use the built-in Pytorch methods to impelement this is a few lines.\\n\\n```python\\nclass PackAttention(nn.Module):\\n    def __init__(...):\\n        super(PackAttention, self).__init__()\\n        self.embed_dim = embed_dim\\n        self.num_heads = num_heads\\n\\n        self.mha = nn.MultiheadAttention(\\n            embed_dim=self.embed_dim, num_heads=self.num_heads, batch_first=True\\n        )\\n\\n    def forward(self, P: torch.Tensor, X: torch.Tensor, attention_mask: torch.Tensor):\\n        attn, _ = self.mha(P, X, X, attention_mask)\\n        return attn\\n```\\n\\n## USWAttention\\n\\nLet us move on to implementing Unpack & Sliding Window Attention.\\n\\n### init\\n\\nAs shown in _Figure 2_ above, Unpack & Sliding Window Attention is composed of the Unpack Attention and Global + Sliding Window Attention.\\nAdditionally, the location information for each part is represented using a uniform distance matrix $D_p$ and BiALiBi $D$ respectively.\\n\\n```python\\nclass UnpackSlidingWindowAttention(nn.Module):\\n    def __init__(...):\\n        super(UnpackSlidingWindowAttention, self).__init__()\\n\\n        # Hyperparameters\\n        self.attn_head_size = int(dim / num_attention_heads)\\n        self.num_attention_heads = num_attention_heads\\n        self.block_size = block_size\\n        self.seq_len = seq_len\\n        self.pack_len = pack_len\\n\\n        # QKV linear transformation layers\\n        self.Q = nn.Linear(dim, self.attn_head_size * num_attention_heads)\\n        self.K = nn.Linear(dim, self.attn_head_size * num_attention_heads)\\n        self.V = nn.Linear(dim, self.attn_head_size * num_attention_heads)\\n\\n        # location information\\n        self.bialibi = BidirectionalALiBi(\\n            self.num_attention_heads, self.seq_len\\n        )\\n        self.uniform_dist_mat = UniformDistanceMatrix(\\n            self.num_attention_heads, self.block_size, self.seq_len, self.pack_len\\n        )\\n\\n\\n        self.register_buffer(\\"middle_band_distance_indicies\\", None, persistent=False)\\n```\\n\\n### Linear Transformations\\n\\nWe start by calculating the scaling factor $\\\\sqrt{d}$, where $d = embed\\\\_size / num\\\\_heads$, since we are also doing multihead attention.\\n\\nNext we perform the linear transformations of $X$ into $Q(X)$, $K(X)$, $V(X)$, and $C_p$ into $K(C_p)$ and $V(C_p)$.\\n\\nWe follow up by splitting the transformations into $n\\\\_heads$, and transposing the matrix such that the last two layers correspond to $seq\\\\_len$ and $d$. \\n\\n\\n```python\\n    def forward(self, X, Cp, ...)\\n\\n        ...\\n\\n        rsqrt_d = 1 / math.sqrt(self.attn_head_size)\\n\\n        batch_size, seq_len, dim = X.shape\\n\\n        query = transpose_for_scores(\\n            self.Q(X), self.num_attention_heads, self.attn_head_size\\n        )  # bsz, head, seq_len, head_dim\\n        key_x = transpose_for_scores(\\n            self.K(X), self.num_attention_heads, self.attn_head_size\\n        )  # bsz, head, seq_len, head_dim\\n        value_x = transpose_for_scores(\\n            self.V(X), self.num_attention_heads, self.attn_head_size\\n        )  # bsz, head, seq_len, head_dim\\n\\n        key_Cp = transpose_for_scores(\\n            self.K(Cp), self.num_attention_heads, self.attn_head_size\\n        )  # bsz, head, pack_len, head_dim\\n        value_Cp = transpose_for_scores(\\n            self.V(Cp), self.num_attention_heads, self.attn_head_size\\n        )  # bsz, head, pack_len, head_dim\\n\\n        ...\\n```\\n\\n:::note\\n\\nSee the impelmentation for `transpose_for_scores` below in the Utility Functions section.\\n\\n:::\\n\\n### Unpack Attention\\n\\nNext we will move on to calculating the Unpack Attention part of Unpack & Sliding Window Attention.\\n\\nIn typical self-attention, one would first calculate the attention scores between the key and query vectors, scale them, and multiply them by the value vectors to get the context vector.\\nOn the other hand, USW Attention calculates the attention scores and contexts using the concatenation of $K(C_p)$ and $K(X)$, and $V(C_p)$ and $V(X)$.\\n\\nWe can achieve this by calculating the sliding attention and unpack attention separately and adding them together in the end.\\n\\nSince the order isn\'t super important, we will start with unpack attention.\\n\\n#### Code\\n\\nNote the line `key_cp_attn[:] -= Dp`. Here we are apply the uniform distance matrix to the attention scores before calculating the context.\\n\\n```python\\n    # Step 1 calculate the attention scores for the packed data\\n    key_cp_attn = torch.matmul(query, key_Cp.transpose(2, 3))\\n    key_cp_attn = key_cp_attn * rsqrt_d\\n    key_cp_attn[:] -= Dp\\n    key_cp_attn = F.softmax(key_cp_attn, dim=-1)  # bsz, heads, seq_len, pack_len\\n\\n    packed_context = torch_bmm_nd(key_cp_attn, value_Cp, ndim=4)\\n```\\n\\n### Global + Sliding Window\\n\\n<Figure>\\n  <img src={full_sparse_attn_img} />\\n  Figure 3: Full Attention\uacfc Global + Sliding Window Attention \ube44\uad50\\n</Figure>\\n\\nDue to the quadratic time complexity of typical self-attention, working with sequence lengths longer than 512 becomes computationally expensive, whereas using a sparse attention method, like the one proposed in LittleBird, one can reduce the time complexity allowing for significantly longer input sequences. \\n\\nThe image above depicts how this would look in comparison to typical self-attention. The white squares represent attention scores that we do not calculate. \\n\\nThe authors mention that with a small enough $block\\\\_size$ and $pack\\\\_len$ the time complexity for LittleBird\'s attention can be considered to have a linear time complexity wrt $seq\\\\_len$. More specifically, $O(l(4b + 2s))$ compared to self-attention\'s $O(n^2d)$.\\n\\nUnfortunately, as it is well known that performing sparse multiplications cannot be efficiently implemented on the GPU, the authors of the BigBird paper proposed blocking the sets of query and keys together and performing the attention calculating between these blocks instead. This will serve as the basis for which we will calculate LittleBird\'s global + sliding window attention.\\n\\n### Blockify the input\\n\\n<Figure>\\n  <img src={block_tokens_ex_img} />\\n  Figure 4: blockify example with a seq_len of 512 and a block_size of 64\\n</Figure>\\n\\n\\nBlockifying the sequence is as simple as calling the view method on the vectors as shown below.\\n\\n```python\\nquery_block = query.view(\\n    batch_size,\\n    self.num_attention_heads,\\n    seq_len // self.block_size,\\n    self.block_size,\\n    -1,\\n)\\nkey_x_block = key_x.view(\\n    batch_size,\\n    self.num_attention_heads,\\n    seq_len // self.block_size,\\n    self.block_size,\\n    -1,\\n)\\nvalue_x_block = value_x.view(\\n    batch_size,\\n    self.num_attention_heads,\\n    seq_len // self.block_size,\\n    self.block_size,\\n    -1,\\n)\\n```\\n\\n### First two rows\\n\\nWe calculate the attention scores in three steps: the first two rows, the middle band, and the last row.\\n\\n<Figure>\\n  <img src={first_two_rows_img} />\\n  Figure 4: Attention for the first two rows\\n</Figure>\\n\\nWe calculate the first two and last rows separately from the middle rows because we want to preserve the order of the blocks in which we calculate attention. This will become more clear in the following sections.\\n\\n#### Code\\n\\nNote that the dimensions for the vectors at this point are: \\n```\\nbatch_size, attn_heads, num_blocks, block_size, head_dim = key_x_block.shape\\n```\\n\\n\\nWe first transpose the key, value vectors such that the first 4 blocks are grouped together, and we transpose the query such that the first two blocks are grouped together.\\n\\n```\\n# Step 2.1. process the first two rows\\nfirst_two_rows_key_matrix = torch.cat(\\n    [\\n        key_x_block[:, :, 0],\\n        key_x_block[:, :, 1],\\n        key_x_block[:, :, 2],\\n        key_x_block[:, :, 3],\\n    ],\\n    dim=2,\\n)\\nfirst_two_rows_value_matrix = torch.cat(\\n    [\\n        value_x_block[:, :, 0],\\n        value_x_block[:, :, 1],\\n        value_x_block[:, :, 2],\\n        value_x_block[:, :, 3],\\n    ],\\n    dim=2,\\n)\\n\\nfirst_two_query_blocks = torch.cat(\\n    [query_block[:, :, 0], query_block[:, :, 1]], dim=2\\n)\\n```\\n\\nNext we:\\n1. calculate the attention scores with the key and query vectors\\n2. scale the attention scores\\n3. subtract the biases from the BiALiBi distance matrix\\n4. take into account the attention mask\\n5. softmax \\n6. and finally calculate the context with the value vectors\\n\\n```\\nfirst_two_rows_attn = torch_bmm_nd_transpose(\\n    first_two_query_blocks, first_two_rows_key_matrix, ndim=4\\n)\\nfirst_two_rows_attn *= rsqrt_d\\nfirst_two_rows_attn -= D[:, : self.block_size * 2, : self.block_size * 4]\\nfirst_two_rows_attn += (1.0 - self.mask_v[:, :, :self.block_size * 2, :self.block_size * 4]) * attn_penalty\\nfirst_two_rows_attn = F.softmax(first_two_rows_attn, dim=-1)\\n\\nfirst_two_rows_context = torch_bmm_nd(\\n    first_two_rows_attn, first_two_rows_value_matrix, ndim=4\\n)\\n```\\n\\nIn the final step, we will concatenate the results from the first two rows, the middle band, and the last two rows to get a final context vector. We will concatenate on the block dimension and as such need to reshape the context vector.\\n\\n```\\n_, __, ftr_3d, ftr_4d = first_two_rows_context.shape\\nfirst_two_rows_context = first_two_rows_context.view(\\n    batch_size, self.num_attention_heads, 2, ftr_3d // 2, ftr_4d\\n)  # bsz, heads, 2(blocks), block_size, block_size*4\\n```\\n\\n### Sliding Window (Middle Band)\\n\\nIn this section, we use the techniques described in the BigBird paper to calculate the attention for the middle band. \\n\\nI also recommend reading the explanation in the BigBird paper or HuggingFace blog<sub>[5]</sub> to supplement my own and maybe fill in any gaps!\\n\\n<Figure>\\n  <img src={middle_rows_img} />\\n  Figure 4: Attention for the middle band\\n</Figure>\\n\\n#### How it Works\\n\\nWe take the blocked key vectors and copy them twice with the elements shifted once to the left and once to the right. If we then multiply the query vectors by the 3 shifted vectors, we can cover all of the sliding tokens.\\n\\nThis works because we\'re multiplying along the blocked dimension. Consider the image below. Multiplying along the block dimension between the keys and query would give us the attention scores for the elements in those blocks only.\\n\\nWhen multiplying the query with the unshifted keys, we get the attention scores for the blocks on the diagnol. If we shift the values to the right and left, and do the same multiplication operation, we get the scores for the blocks just left and right of the diagnol. \\n\\n<Figure>\\n  <img src={sliding_window_broken_down} />\\n  Figure 4: The three steps that go into calculating the sliding window attention\\n</Figure>\\n\\n\\n#### Sliding Window implementation\\n\\n```\\n# step 2.2 calculate the middle part of the matrix\\n# the trick described in the bigbird paper is used\\n\\nmiddle_band_key_matrix = torch.cat(\\n    [\\n        key_x_block[:, :, 1:-2],  # roll back one\\n        key_x_block[:, :, 2:-1],\\n        key_x_block[:, :, 3:],  # roll forward one\\n    ],\\n    dim=3,\\n)\\nmiddle_band_value_matrix = torch.cat(\\n    [\\n        value_x_block[:, :, 1:-2],  # roll back one\\n        value_x_block[:, :, 2:-1],\\n        value_x_block[:, :, 3:],  # roll forward one\\n    ],\\n    dim=3,\\n)\\n\\n\\n\\n# get the diagnol\\nmiddle_band_sliding = torch_bmm_nd_transpose(\\n    query_block[:, :, 2:-1], middle_band_key_matrix, ndim=5\\n)\\nmiddle_band_sliding += (1.0 - self.band_mask) * attn_penalty\\n```\\n\\n\\n#### Global Attention Implementation\\n\\nLittleBird also calculates a global attention by attending the first block to every other token in the sequence. This is represented by the orange strip in the _Global + Sliding Window_ image above. We can calculate this separately and concatenate it with the result of the sliding window operation.\\n\\n```\\n# get the global\\nmiddle_band_global = torch.einsum(\\n    \\"bhlqd,bhkd->bhlqk\\", query_block[:, :, 2:-1], key_x_block[:, :, 0]\\n)\\nmiddle_band_global += (1.0 - self.mask_block[:,2:-1,:].unsqueeze(3)) * attn_penalty\\n\\nmiddle_band_attn = torch.cat([middle_band_global, middle_band_sliding], dim=-1)\\nmiddle_band_attn *= rsqrt_d\\nmiddle_band_attn -= self.get_middle_band_distances(D)\\nmiddle_band_attn = F.softmax(middle_band_attn, dim=-1)\\n\\nmiddle_band_context = torch.einsum(\\n    \\"bhlqk,bhkd->bhlqd\\",\\n    middle_band_attn[:, :, :, :, : self.block_size],\\n    value_x_block[:, :, 0],\\n)\\nmiddle_band_context += torch_bmm_nd(\\n    middle_band_attn[:, :, :, :, self.block_size : 4 * self.block_size],\\n    middle_band_value_matrix,\\n    ndim=5,\\n)\\n```\\n\\n### Last Row\\n\\n<Figure>\\n  <img src={last_row_img} />\\n  Figure 4: Attention for the last row\\n</Figure>\\n\\nThe last row is done similarly to the first two rows but we use the first block and the last three blocks, as opposed to the first four blocks, for the key and value vectors.\\n\\n```\\n# calcualte the last row\\nlast_row_key_matrix = torch.cat(\\n    [\\n        key_x_block[:, :, 0],\\n        key_x_block[:, :, -3],\\n        key_x_block[:, :, -2],\\n        key_x_block[:, :, -1],\\n    ],\\n    dim=2,\\n)\\nlast_row_value_matrix = torch.cat(\\n    [\\n        value_x_block[:, :, 0],\\n        value_x_block[:, :, -3],\\n        value_x_block[:, :, -2],\\n        value_x_block[:, :, -1],\\n    ],\\n    dim=2,\\n)\\n\\nlast_row_attn = torch_bmm_nd_transpose(\\n    query_block[:, :, -1], last_row_key_matrix, ndim=4\\n)\\nlast_row_attn *= rsqrt_d\\nlast_row_attn -= D[:, -self.block_size :, -self.block_size * 4 :]\\nlast_row_attn = F.softmax(last_row_attn, dim=-1)\\n\\nlast_row_context = torch_bmm_nd(last_row_attn, last_row_value_matrix, ndim=4)\\nlast_row_context.unsqueeze_(2)\\n```\\n\\n### Bringing everything together\\n\\nTo finish up, we concatenate the contexts from the first two rows, the middle band, and the final row; do some reshaping; add the packed and sliding window context; and reshape the final context vector.\\n\\n```\\ncontext_layer = torch.cat(\\n    [first_two_rows_context, middle_band_context, last_row_context], dim=2\\n)\\ncontext_layer = context_layer.view(\\n    (batch_size, self.num_attention_heads, seq_len, -1)\\n)\\n\\nCx = context_layer + packed_context\\nCx = Cx.view(\\n    batch_size, seq_len, self.num_attention_heads * self.attn_head_size\\n) * self.mask_v.squeeze(1)\\n\\nreturn Cx\\n```\\n\\n## BiALiBi\\n\\nWe can achieve the BiALiBi matrix by breaking it down into largely three steps. We initialize a vector representing the absolute distances from the diagnol; calculate masks filled with the trainable \u03b3, \u03b2, and \u03b1 values; and finish by performing elementwise multiplciation between the distance vector and the masks.\\n\\n#### Step 1. Create the base distance matrix\\n\\n```Python\\nrow_i = torch.arange(self.seq_len, dtype=torch.float32)\\ncol_i = torch.arange(self.seq_len, dtype=torch.float32).unsqueeze(-1)\\ndistances = (row_i - col_i).abs()\\ndistances\\n\\ntensor([[0, 1, 2, 3, 4, 5],\\n        [1, 0, 1, 2, 3, 4],\\n        [2, 1, 0, 1, 2, 3],\\n        [3, 2, 1, 0, 1, 2],\\n        [4, 3, 2, 1, 0, 1],\\n        [5, 4, 3, 2, 1, 0]], dtype=torch.int32)\\n```\\n\\n#### Step 2. Create the gamma(\u03b3) and beta(\u03b2) masks\\n\\n```\\ngamma_mask = torch.triu(torch.ones_like(self.distances), diagonal=1)\\ngamma_mask *= self.gamma.view(-1, 1, 1)\\ngamma_mask\\n\\ntensor([[[0.0000, 0.4540, 0.4540, 0.4540, 0.4540, 0.4540],\\n         [0.0000, 0.0000, 0.4540, 0.4540, 0.4540, 0.4540],\\n         [0.0000, 0.0000, 0.0000, 0.4540, 0.4540, 0.4540],\\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.4540, 0.4540],\\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4540],\\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\\n       grad_fn=<MulBackward0>)\\n```\\n\\n```\\nbeta_mask = torch.tril(torch.ones_like(self.distances), diagonal=-1)\\nbeta_mask *= self.beta.view(-1, 1, 1)\\nbeta_mask\\n\\ntensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\\n         [0.9392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\\n         [0.9392, 0.9392, 0.0000, 0.0000, 0.0000, 0.0000],\\n         [0.9392, 0.9392, 0.9392, 0.0000, 0.0000, 0.0000],\\n         [0.9392, 0.9392, 0.9392, 0.9392, 0.0000, 0.0000],\\n         [0.9392, 0.9392, 0.9392, 0.9392, 0.9392, 0.0000]]],\\n       grad_fn=<MulBackward0>)\\n```\\n\\n\\n#### Step 3. Add the masks and insert the alpha(\u03b1) parameter.\\n\\n```Python\\nmask = beta_mask + gamma_mask\\n\\n# step 4: set the alphas\\nmask[:, 0, :] = 1.0\\nmask[:, :, 0] = 1.0\\nmask[:, 1:, 0] *= self.alpha.unsqueeze(1)\\nmask[:, 0, 1:] *= self.alpha.unsqueeze(1)\\nmask[:, 0, 0] *= 0.0\\n\\ntensor([[[0.0000, 0.7959, 0.7959, 0.7959, 0.7959, 0.7959],\\n         [0.7959, 0.0000, 0.4540, 0.4540, 0.4540, 0.4540],\\n         [0.7959, 0.9392, 0.0000, 0.4540, 0.4540, 0.4540],\\n         [0.7959, 0.9392, 0.9392, 0.0000, 0.4540, 0.4540],\\n         [0.7959, 0.9392, 0.9392, 0.9392, 0.0000, 0.4540],\\n         [0.7959, 0.9392, 0.9392, 0.9392, 0.9392, 0.0000]]],\\n       grad_fn=<CopySlices>)\\n```\\n\\nNow we can finish off by just multiplying the base distance matrix with the mask.\\n\\n```\\nself.distances * mask\\n\\ntensor([[[0.0000, 0.2621, 0.5243, 0.7864, 1.0486, 1.3107],\\n         [0.2621, 0.0000, 0.6620, 1.3239, 1.9859, 2.6478],\\n         [0.5243, 0.4262, 0.0000, 0.6620, 1.3239, 1.9859],\\n         [0.7864, 0.8524, 0.4262, 0.0000, 0.6620, 1.3239],\\n         [1.0486, 1.2787, 0.8524, 0.4262, 0.0000, 0.6620],\\n         [1.3107, 1.7049, 1.2787, 0.8524, 0.4262, 0.0000]]],\\n       grad_fn=<MulBackward0>)\\n```\\n\\n\\n## Utility functions\\n\\nBelow are functions borrowed and slightly modified from HuggingFace\'s implementation of BigBird.\\n\\n```\\ndef torch_bmm_nd_transpose(inp_1, inp_2, ndim=None):\\n    \\"\\"\\"Fast nd matrix multiplication with transpose\\"\\"\\"\\n    # faster replacement of torch.einsum (bhqd,bhkd->bhqk)\\n    return torch.bmm(\\n        inp_1.reshape((-1,) + inp_1.shape[-2:]),\\n        inp_2.reshape((-1,) + inp_2.shape[-2:]).transpose(1, 2),\\n    ).view(inp_1.shape[: ndim - 2] + (inp_1.shape[ndim - 2], inp_2.shape[ndim - 2]))\\n\\n\\ndef torch_bmm_nd(inp_1, inp_2, ndim=None):\\n    \\"\\"\\"Fast nd matrix multiplication\\"\\"\\"\\n    # faster replacement of torch.einsum (\\"bhqk,bhkd->bhqd\\")\\n    return torch.bmm(\\n        inp_1.reshape((-1,) + inp_1.shape[-2:]), inp_2.reshape((-1,) + inp_2.shape[-2:])\\n    ).view(inp_1.shape[: ndim - 2] + (inp_1.shape[ndim - 2], inp_2.shape[ndim - 1]))\\n\\n\\ndef transpose_for_scores(x, num_attn_head, attn_head_size):\\n    new_x_shape = x.size()[:-1] + (num_attn_head, attn_head_size)\\n    x = x.view(*new_x_shape)\\n    return x.permute(0, 2, 1, 3)\\n```\\n\\n## Additional Info\\n\\nThank you for reading.\\n\\nYou may view the full source code on [Github](https://github.com/jwnz/littlebird).\\n\\nIf you have any comments regarding this explanation or perhaps I have any mistakes in my impelmentation, please feel free to open an issue on the Github repository above! \ud83d\ude0a\\n\\n## References\\n\\n- [[1] LittleBird: Efficient Faster & Longer Transformer for Question Answering](https://arxiv.org/abs/2210.11870)\\n- [[2] Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)\\n- [[3] Luna: Linear Unified Nested Attention](https://arxiv.org/abs/2106.01540)\\n- [[4] Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)\\n- [[5] Understanding BigBird\'s Block Sparse Attention](https://huggingface.co/blog/big-bird)"}]}')}}]);